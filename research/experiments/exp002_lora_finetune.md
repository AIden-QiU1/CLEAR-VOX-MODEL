# EXP-002: LoRAå¾®è°ƒ

> **çŠ¶æ€**: ğŸ”„ è®¡åˆ’ä¸­  
> **ä¼˜å…ˆçº§**: P0  
> **ä¾èµ–**: EXP-001  
> **é¢„è®¡æ—¶é—´**: 3-5å¤©

---

## å‡è®¾

ä½¿ç”¨LoRAå¯¹Paraformer-largeçš„Encoderè¿›è¡Œå¾®è°ƒï¼ŒåŒæ—¶å†»ç»“Decoderï¼Œå¯ä»¥åœ¨CDSDæ•°æ®ä¸Šè·å¾—æ˜¾è‘—çš„CERæå‡ã€‚

## æ–¹æ³•

1. åœ¨Encoderçš„self-attentionå±‚æ·»åŠ LoRAé€‚é…å™¨
2. å†»ç»“Decoderæ‰€æœ‰å‚æ•°
3. ä½¿ç”¨CDSD 10hæ•°æ®è®­ç»ƒ
4. å¯¹æ¯”ä¸åŒLoRA rankçš„æ•ˆæœ

## é…ç½®

```yaml
model:
  name: paraformer-large
  source: modelscope
  
lora:
  enabled: true
  rank: 8
  alpha: 16
  target_modules:
    - encoder.encoders.*.self_attn.linear_q
    - encoder.encoders.*.self_attn.linear_v
  dropout: 0.1

training:
  epochs: 30
  batch_size: 8
  gradient_accumulation: 4
  learning_rate: 1e-4
  warmup_steps: 500
  
  freeze:
    decoder: true
    encoder: false
    
  optimizer: AdamW
  scheduler: cosine
  
data:
  train: data/cdsd/10h/train
  val: data/cdsd/10h/val
  test: data/cdsd/10h/test

device:
  gpu: 0
  mixed_precision: fp16
```

## æ‰§è¡Œå‘½ä»¤

```bash
# è®­ç»ƒ
bash scripts/finetune_paraformer_10h_optimized.sh

# è¯„ä¼°
python scripts/inference_finetuned.py \
    --model_path outputs/exp002/checkpoint_best \
    --test_data data/cdsd/10h/test/wav.scp
```

## æ¶ˆèå®éªŒ

| å®éªŒID | LoRA rank | å†»ç»“ç­–ç•¥ | é¢„æœŸCER |
|--------|-----------|----------|---------|
| 002a | 4 | å†»ç»“Decoder | - |
| 002b | 8 | å†»ç»“Decoder | - |
| 002c | 16 | å†»ç»“Decoder | - |
| 002d | 8 | å…¨é‡è®­ç»ƒ | - |

## é¢„æœŸç»“æœ

| æŒ‡æ ‡ | åŸºçº¿ | ç›®æ ‡ | æå‡ |
|------|------|------|------|
| CER | ~50% | ~30% | 40% rel. |

## å®é™…ç»“æœ

| å®éªŒID | CER | è®­ç»ƒæ—¶é—´ | æ˜¾å­˜ |
|--------|-----|----------|------|
| 002a | - | - | - |
| 002b | - | - | - |

## åˆ†æ

ï¼ˆå¾…å®éªŒå®Œæˆåå¡«å†™ï¼‰

## ä¸‹ä¸€æ­¥

- [ ] å®ŒæˆLoRAå¾®è°ƒ
- [ ] é€‰æ‹©æœ€ä½³rank
- [ ] è¿›å…¥ EXP-003 æ•°æ®å¢å¼º
