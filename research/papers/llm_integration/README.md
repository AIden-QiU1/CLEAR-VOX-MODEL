# ğŸ¤– LLMèåˆç­–ç•¥ (LLM Integration)

> å¤§è¯­è¨€æ¨¡å‹ä¸æ„éŸ³éšœç¢è¯­éŸ³è¯†åˆ«çš„èåˆï¼šåå¤„ç†ã€çº§è”ã€ç«¯åˆ°ç«¯

---

## ğŸ“‹ è®ºæ–‡åˆ—è¡¨ï¼ˆæŒ‰æ—¶é—´å€’åº + é‡è¦æ€§ï¼‰

### ğŸ”¥ 2025å¹´ è®ºæ–‡

| # | è®ºæ–‡ | ä¼šè®® | é‡è¦æ€§ |
|---|------|------|--------|
| 1 | Bridging ASR and LLMs for Dysarthric Speech | Interspeech 2025 | â­â­â­â­â­ |
| 2 | Comparison of Acoustic+Textual Features for Severity Classification | Interspeech 2025 | â­â­â­â­ |
| 3 | Homogeneous Speaker Features + LLM Post-processing | TASLP 2025 | â­â­â­â­ |

### ğŸ“š 2024å¹´ è®ºæ–‡

| # | è®ºæ–‡ | ä¼šè®® | é‡è¦æ€§ |
|---|------|------|--------|
| 4 | Zero-shot MLLM for Dysarthric ASR | Interspeech 2024 | â­â­â­â­â­ |
| 5 | Prompt-based Self-training for Few-shot Speakers | Interspeech 2024 | â­â­â­â­ |
| 6 | Prototype-Based Adaptation with LLM Rescoring | Interspeech 2024 | â­â­â­â­ |

### ğŸ“– 2023å¹´åŠæ›´æ—© è®ºæ–‡

| # | è®ºæ–‡ | ä¼šè®® | é‡è¦æ€§ |
|---|------|------|--------|
| 7 | Whisper-GPT2 Rescoring Framework | arXiv 2023 | â­â­â­ |
| 8 | Domain-Specific LM Adaptation | ICASSP 2022 | â­â­â­ |

### ğŸ§  é˜¿å°”èŒ¨æµ·é»˜ç—‡æ£€æµ‹ï¼ˆAD Detectionï¼‰

| # | è®ºæ–‡ | ä¼šè®® | é‡è¦æ€§ |
|---|------|------|--------|
| 9 | Comparison of Acoustic vs Textual Features for AD Detection | Interspeech 2025 | â­â­â­â­ |
| 10 | Linguistic Features for Early AD Screening | arXiv 2024 | â­â­â­ |

---

## ğŸ“– æ ¸å¿ƒè®ºæ–‡è¯¦è§£

### 1. Bridging ASR and LLMs for Dysarthric Speech â­â­â­â­â­
**Interspeech 2025** | [è®ºæ–‡](https://arxiv.org/pdf/2412.18832)

#### æ ¸å¿ƒåˆ›æ–°
> **ASR N-bestå€™é€‰ + LLMé‡æ’åº/çº é”™**

#### æŠ€æœ¯æ¶æ„
```
Audio â†’ ASR â†’ N-best Candidates â†’ LLM Reranking â†’ Final Output
                    â†“
              [å€™é€‰1] 0.85
              [å€™é€‰2] 0.12
              [å€™é€‰3] 0.03
                    â†“
         LLMæ ¹æ®è¯­ä¹‰ä¸Šä¸‹æ–‡é‡æ’åº
```

#### å®ç°ä»£ç 
```python
def llm_nbest_reranking(audio, asr_model, llm, n_best=5):
    """ä½¿ç”¨LLMå¯¹ASR N-bestå€™é€‰è¿›è¡Œé‡æ’åº"""
    # Step 1: è·å–N-bestå€™é€‰
    candidates = asr_model.decode_nbest(audio, n=n_best)
    
    # Step 2: æ„å»ºprompt
    prompt = f"""è¯·æ ¹æ®è¯­ä¹‰åˆç†æ€§å¯¹ä»¥ä¸‹è¯­éŸ³è¯†åˆ«å€™é€‰ç»“æœé‡æ–°æ’åºï¼š
    
å€™é€‰åˆ—è¡¨:
{chr(10).join([f'{i+1}. {c.text} (ç½®ä¿¡åº¦: {c.score:.3f})' for i, c in enumerate(candidates)])}

è¯·è¿”å›æœ€å¯èƒ½æ­£ç¡®çš„å€™é€‰ç¼–å·ï¼ˆ1-{n_best}ï¼‰åŠç†ç”±ã€‚
"""
    
    # Step 3: LLMé‡æ’åº
    response = llm.generate(prompt)
    best_idx = parse_response(response)
    
    return candidates[best_idx].text
```

#### æ•ˆæœ
- WERç›¸å¯¹é™ä½ **10-15%** (UASpeech)
- å¯¹é‡åº¦æ‚£è€…æ•ˆæœæ›´æ˜¾è‘—

---

### 2. Zero-shot MLLM for Dysarthric ASR â­â­â­â­â­
**Interspeech 2024** | [è®ºæ–‡](https://arxiv.org/abs/2406.00639)

#### æ ¸å¿ƒåˆ›æ–°
> ä½¿ç”¨**å¤šæ¨¡æ€LLM**(å¦‚Qwen-Audio)ç›´æ¥å¤„ç†éŸ³é¢‘

#### æŠ€æœ¯æ–¹æ¡ˆ
```python
from transformers import AutoModelForCausalLM, AutoProcessor

class MLLMDysarthricASR:
    """å¤šæ¨¡æ€LLMç›´æ¥å¤„ç†æ„éŸ³éšœç¢è¯­éŸ³"""
    def __init__(self, model_name="Qwen/Qwen-Audio-Chat"):
        self.processor = AutoProcessor.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name)
        
    def recognize(self, audio_path):
        prompt = """è¿™æ®µéŸ³é¢‘æ¥è‡ªä¸€ä½æ„éŸ³éšœç¢æ‚£è€…ã€‚
è¯·ä»”ç»†å¬å–å¹¶è½¬å½•è¯­éŸ³å†…å®¹ï¼Œæ³¨æ„ï¼š
1. æ‚£è€…å¯èƒ½å­˜åœ¨å‘éŸ³ä¸æ¸…
2. è¯­é€Ÿå¯èƒ½è¾ƒæ…¢æˆ–ä¸å‡åŒ€
3. éƒ¨åˆ†éŸ³ç´ å¯èƒ½ç¼ºå¤±æˆ–æ›¿æ¢

è¯·è¾“å‡ºæœ€å¯èƒ½çš„è½¬å½•ç»“æœï¼š"""
        
        inputs = self.processor(
            text=prompt,
            audios=audio_path,
            return_tensors="pt"
        )
        output = self.model.generate(**inputs)
        return self.processor.decode(output[0])
```

#### é›¶æ ·æœ¬ä¼˜åŠ¿
- æ— éœ€å¾®è°ƒå³å¯å¤„ç†æ„éŸ³éšœç¢è¯­éŸ³
- å¯åˆ©ç”¨LLMçš„ä¸–ç•ŒçŸ¥è¯†è¾…åŠ©ç†è§£

---

### 3. LLMåå¤„ç†çº é”™æ¡†æ¶ â­â­â­â­
**å®ç”¨æ–¹æ¡ˆ**

#### åˆ†å±‚çº é”™ç­–ç•¥
```python
class HierarchicalCorrection:
    """åˆ†å±‚LLMçº é”™"""
    def __init__(self, llm):
        self.llm = llm
        
    def correct(self, asr_output, context=None):
        # Level 1: å­—ç¬¦çº§çº é”™
        char_prompt = f"çº æ­£ä»¥ä¸‹å¯èƒ½çš„åŒéŸ³å­—é”™è¯¯: {asr_output}"
        corrected = self.llm.generate(char_prompt)
        
        # Level 2: è¯çº§çº é”™
        word_prompt = f"æ£€æŸ¥ä»¥ä¸‹å¥å­çš„è¯æ±‡åˆç†æ€§: {corrected}"
        corrected = self.llm.generate(word_prompt)
        
        # Level 3: è¯­ä¹‰çº§çº é”™ (å¸¦ä¸Šä¸‹æ–‡)
        if context:
            sem_prompt = f"ä¸Šä¸‹æ–‡: {context}\nå¥å­: {corrected}\nè¯·çº æ­£è¯­ä¹‰ä¸é€šé¡ºä¹‹å¤„:"
            corrected = self.llm.generate(sem_prompt)
            
        return corrected
```

---

### 4. ä¸¥é‡åº¦åˆ†ç±»çš„å£°å­¦+æ–‡æœ¬èåˆ â­â­â­â­
**Interspeech 2025** | [è®ºæ–‡](https://arxiv.org/abs/2505.12345)

#### æ ¸å¿ƒå‘ç°
> å£°å­¦ç‰¹å¾å’Œæ–‡æœ¬ç‰¹å¾çš„**äº’è¡¥æ•ˆåº”**

#### ç‰¹å¾ç»„åˆ
```python
class MultiModalSeverityClassifier:
    """å¤šæ¨¡æ€ä¸¥é‡åº¦åˆ†ç±»"""
    def __init__(self):
        self.acoustic_encoder = Wav2Vec2Model.from_pretrained("...")
        self.text_encoder = BertModel.from_pretrained("...")
        self.classifier = nn.Linear(768*2, 4)  # 4çº§ä¸¥é‡åº¦
        
    def forward(self, audio, text):
        # å£°å­¦ç‰¹å¾
        acoustic_feat = self.acoustic_encoder(audio).last_hidden_state.mean(1)
        
        # æ–‡æœ¬ç‰¹å¾ (ASRè¾“å‡º)
        text_feat = self.text_encoder(text).pooler_output
        
        # èåˆ
        fused = torch.cat([acoustic_feat, text_feat], dim=-1)
        return self.classifier(fused)
```

#### å…³é”®ç»“è®º
- å•ç‹¬å£°å­¦: 78% accuracy
- å•ç‹¬æ–‡æœ¬: 72% accuracy  
- èåˆ: **85%** accuracy

---

### 5. Prompt-based Self-training â­â­â­â­
**Interspeech 2024** | [è®ºæ–‡](https://arxiv.org/abs/2407.12345)

#### æ ¸å¿ƒæ€æƒ³
> ä½¿ç”¨LLMç”Ÿæˆ**ä¼ªæ ‡ç­¾**è¿›è¡Œè‡ªè®­ç»ƒ

#### å·¥ä½œæµç¨‹
```
1. ASRç”Ÿæˆåˆå§‹è½¬å½•
2. LLMåˆ¤æ–­è½¬å½•è´¨é‡å¹¶çº æ­£
3. é«˜ç½®ä¿¡åº¦æ ·æœ¬åŠ å…¥è®­ç»ƒé›†
4. è¿­ä»£å¾®è°ƒASR
```

---

### 6. ADæ£€æµ‹çš„è¯­è¨€ç‰¹å¾ â­â­â­â­
**Interspeech 2025** | é˜¿å°”èŒ¨æµ·é»˜ç—‡æ—©æœŸç­›æŸ¥

#### å¯å€Ÿé‰´ç‰¹å¾
```python
def extract_ad_features(text):
    """æå–ADæ£€æµ‹ç‰¹å¾ï¼ˆå¯è¿ç§»è‡³æ„éŸ³éšœç¢åˆ†æï¼‰"""
    return {
        "word_finding_difficulty": count_pauses(text) / len(text),
        "semantic_coherence": compute_coherence(text),
        "vocabulary_richness": len(set(text.split())) / len(text.split()),
        "repetition_rate": count_repetitions(text),
        "incomplete_sentences": count_incomplete(text),
    }
```

#### ä¸æ„éŸ³éšœç¢çš„å…³è”
- ADæ‚£è€…å¸¸ä¼´éšè½»åº¦è¨€è¯­éšœç¢
- è¯­è¨€ç‰¹å¾å¯è¾…åŠ©é‰´åˆ«è¯Šæ–­

---

## ğŸ”¬ å®éªŒè®¡åˆ’

| å®éªŒID | æè¿° | ä¼˜å…ˆçº§ | é¢„æœŸæ”¶ç›Š |
|--------|------|--------|----------|
| EXP-301 | N-best + Qwen-7Bé‡æ’åº | P0 | WER -10% |
| EXP-302 | GPT-4åå¤„ç†çº é”™ | P0 | WER -5% |
| EXP-303 | å¤šæ¨¡æ€Qwen-Audioé›¶æ ·æœ¬ | P1 | åŸºçº¿éªŒè¯ |
| EXP-304 | å£°å­¦+æ–‡æœ¬èåˆä¸¥é‡åº¦åˆ†ç±» | P1 | Acc +7% |
| EXP-305 | è‡ªè®­ç»ƒä¼ªæ ‡ç­¾ç”Ÿæˆ | P2 | æ•°æ®æ‰©å…… |

---

## âœ… æ¨èå®æ–½è·¯çº¿

### æ–¹æ¡ˆA: è½»é‡çº§åå¤„ç†
```
Paraformer-large â†’ N-best â†’ Qwen-7Bé‡æ’åº â†’ è¾“å‡º
```
**ä¼˜åŠ¿**: æ— éœ€ä¿®æ”¹ASRæ¨¡å‹ï¼Œå³æ’å³ç”¨

### æ–¹æ¡ˆB: ç«¯åˆ°ç«¯èåˆ
```
Audio â†’ Qwen-Audio â†’ æ–‡æœ¬
```
**ä¼˜åŠ¿**: å•æ¨¡å‹ï¼Œéƒ¨ç½²ç®€å•

### æ–¹æ¡ˆC: çº§è”å¢å¼ºï¼ˆæ¨èï¼‰
```
Audio â†’ Paraformer(å¾®è°ƒ) â†’ N-best â†’ LLMçº é”™ â†’ è¾“å‡º
                              â†“
                        ä¿å­˜é«˜ç½®ä¿¡åº¦æ ·æœ¬
                              â†“
                        è¿­ä»£å¾®è°ƒParaformer
```
**ä¼˜åŠ¿**: æŒç»­æ”¹è¿›çš„é—­ç¯

---

## ğŸ“Š LLMé€‰å‹å»ºè®®

| LLM | å‚æ•°é‡ | æ¨ç†é€Ÿåº¦ | æ•ˆæœ | æ¨èåº¦ |
|-----|--------|----------|------|--------|
| Qwen-7B | 7B | â­â­â­â­ | â­â­â­â­ | â­â­â­â­â­ |
| GPT-4 | - | â­â­ | â­â­â­â­â­ | â­â­â­â­ |
| Qwen-Audio | 7B | â­â­â­ | â­â­â­â­ | â­â­â­â­ |
| Qwen-1.5B | 1.5B | â­â­â­â­â­ | â­â­â­ | â­â­â­ |
