# ğŸ“Š æ„éŸ³éšœç¢è¯­éŸ³è¯†åˆ« (Dysarthric ASR) æŠ€æœ¯è°ƒç ”æŠ¥å‘Š

> **CLEAR-VOX Project Research Report**  
> ç‰ˆæœ¬: 1.0 | æ—¥æœŸ: 2025-12-23  
> ä½œè€…: CLEAR-VOX Research Team

---

## ğŸ“‹ ç›®å½•

1. [æ¨¡å‹ç‰ˆæœ¬ä¸ç°çŠ¶](#1-æ¨¡å‹ç‰ˆæœ¬ä¸ç°çŠ¶)
2. [2024-2025 æœ€æ–°ç ”ç©¶è¿›å±•](#2-2024-2025-æœ€æ–°ç ”ç©¶è¿›å±•)
3. [å¤§è¯­è¨€æ¨¡å‹åœ¨æ„éŸ³éšœç¢ASRçš„åº”ç”¨](#3-å¤§è¯­è¨€æ¨¡å‹åœ¨æ„éŸ³éšœç¢asrçš„åº”ç”¨)
4. [ä¸»æµè®­ç»ƒç­–ç•¥å¯¹æ¯”](#4-ä¸»æµè®­ç»ƒç­–ç•¥å¯¹æ¯”)
5. [é’ˆå¯¹CDSD+Paraformerçš„ä¼˜åŒ–å»ºè®®](#5-é’ˆå¯¹cdsdparaformerçš„ä¼˜åŒ–å»ºè®®)
6. [å‚è€ƒæ–‡çŒ®](#6-å‚è€ƒæ–‡çŒ®)

---

## 1. æ¨¡å‹ç‰ˆæœ¬ä¸ç°çŠ¶

### 1.1 Paraformer-large æ¨¡å‹ä¿¡æ¯

| å±æ€§ | å€¼ |
|------|-----|
| **æ¨¡å‹ID** | `iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch` |
| **å½“å‰ç‰ˆæœ¬** | v2.0.4 (æœ€æ–°ç¨³å®šç‰ˆ) |
| **å‘å¸ƒæ—¶é—´** | 2024-02-01 |
| **å‚æ•°é‡** | 220M |
| **é¢„è®­ç»ƒæ•°æ®** | 60,000+ å°æ—¶ä¸­æ–‡è¯­éŸ³ |
| **ä¸‹è½½é‡** | 32,005,526+ æ¬¡ |

**ç»“è®º**: è¿™æ˜¯ç›®å‰æœ€æ–°çš„å®˜æ–¹ç‰ˆæœ¬ï¼Œæ— æ›´æ–°ç‰ˆæœ¬å‘å¸ƒã€‚

### 1.2 Paraformer å‘å±•å†ç¨‹

| æ—¶é—´ | äº‹ä»¶ |
|------|------|
| 2022å¹´6æœˆ | Paraformer è®ºæ–‡å‘è¡¨ (INTERSPEECH 2022) |
| 2023å¹´1æœˆ | FunASR v0.1.6: Paraformer-large å¼€æº |
| 2023å¹´3æœˆ | FunASR v0.3.0: æµå¼æ¨¡å‹æ”¯æŒ |
| 2024å¹´2æœˆ | v2.0.4: å½“å‰æœ€æ–°ç‰ˆæœ¬ |

### 1.3 æ¶æ„ç»„æˆ

```
Paraformer-large æ¶æ„ (220M å‚æ•°)
â”œâ”€â”€ SANMEncoder (158M, 72%)     - 50å±‚ SANM è‡ªæ³¨æ„åŠ›
â”œâ”€â”€ ParaformerSANMDecoder (61M, 28%) - 16å±‚éè‡ªå›å½’è§£ç 
â”œâ”€â”€ CifPredictorV2 (0.8M)       - CIF é•¿åº¦é¢„æµ‹
â””â”€â”€ SpecAugLFR                  - é¢‘è°±æ•°æ®å¢å¼º
```

---

## 2. 2024-2025 æœ€æ–°ç ”ç©¶è¿›å±•

### 2.1 é‡è¦è®ºæ–‡æ±‡æ€»

#### ğŸ”¥ [2025.12] Zero-Shot Recognition using MLLM
**arXiv:2512.17474** - Ali Alsayegh et al.

> **æ ¸å¿ƒå‘ç°**: 
> - è¯„æµ‹ 8 ä¸ªå•†ç”¨ ASR ç³»ç»Ÿåœ¨ TORGO æ„éŸ³éšœç¢æ•°æ®é›†çš„è¡¨ç°
> - è½»åº¦æ„éŸ³éšœç¢: WER 3-5% (æ¥è¿‘æ­£å¸¸è¯­éŸ³)
> - é‡åº¦æ„éŸ³éšœç¢: WER > 49%
> - **GPT-4o ä½¿ç”¨ verbatim-transcription prompt å¯é™ä½ 7.36% WER**

**å¯¹æœ¬é¡¹ç›®çš„å¯ç¤º**: å¯ä»¥å°è¯•ä½¿ç”¨ LLM åå¤„ç† ASR è¾“å‡ºç»“æœã€‚

---

#### ğŸ”¥ [2024.07] Prototype-Based Adaptation for Unseen Speakers
**arXiv:2407.18461** - INTERSPEECH 2024

> **æ ¸å¿ƒæ–¹æ³•**:
> - ä½¿ç”¨ HuBERT ç‰¹å¾æå–å™¨
> - æ„å»º per-word prototypes (åŸå‹)
> - ç›‘ç£å¯¹æ¯”å­¦ä¹ ä¼˜åŒ–ç‰¹å¾
> - **æ— éœ€å¾®è°ƒå³å¯é€‚åº”æ–°è¯´è¯äºº**

**å¼€æºä»£ç **: https://github.com/NKU-HLT/PB-DSR

**å¯¹æœ¬é¡¹ç›®çš„å¯ç¤º**: å¯ä»¥è€ƒè™‘åŸºäºåŸå‹çš„é€‚åº”ç­–ç•¥ã€‚

---

#### ğŸ”¥ [2024] Fine-Tuning Strategies for Dutch DSR
**INTERSPEECH 2024** - Leivaditi et al.

> **æ ¸å¿ƒå‘ç°** (è·å…°è¯­æ„éŸ³éšœç¢):
> - æ¯”è¾ƒ 3 ç§å¾®è°ƒç­–ç•¥:
>   1. Healthy speech â†’ Dysarthric speech
>   2. Disease-specific data only
>   3. Speaker-specific adaptation
> - **Speaker-specific æ•ˆæœæœ€å¥½ï¼Œä½†æ•°æ®éœ€æ±‚é«˜**
> - **Self-supervised learning (SSL) é¢„è®­ç»ƒç‰¹å¾æœ‰å¸®åŠ©**

---

#### ğŸ”¥ [2024] Speech Technology for DSR: An Overview
**Journal of Speech, Language, and Hearing Research, 2025**

> **ç»¼è¿°è¦ç‚¹**:
> - Transfer Learning (TL) æ˜¯æœ€æœ‰æ•ˆçš„æŠ€æœ¯
> - ä¸åŒ source domain æ•ˆæœå·®å¼‚å¤§
> - æ•°æ®å¢å¼ºå¯ä»¥ç¼“è§£æ•°æ®ç¨€ç¼ºé—®é¢˜
> - **æ˜¾è‘—é™ä½ WER çš„å…³é”®æ˜¯é’ˆå¯¹æ€§çš„é¢†åŸŸé€‚åº”**

---

#### ğŸ”¥ [2024] SLT 2024 LRDWWS Challenge
**IEEE SLT 2024** - æ„éŸ³éšœç¢å”¤é†’è¯æ£€æµ‹æŒ‘æˆ˜èµ›

> **èƒŒæ™¯**: LRDWWS = Low-Resource Dysarthric Wake Word Spotting
> - ä¸“é—¨é’ˆå¯¹æ„éŸ³éšœç¢çš„ç«¯åˆ°ç«¯æ–¹æ³•
> - ä½¿ç”¨ CDSD æ•°æ®é›†çš„å­é›†
> - ç«¯åˆ°ç«¯æ–¹æ³•ä¼˜äºä¼ ç»Ÿçº§è”æ–¹æ³•

---

### 2.2 å¤§è¯­è¨€æ¨¡å‹åº”ç”¨

| æ–¹æ³• | æ¨¡å‹ | WER (è½»åº¦) | WER (é‡åº¦) | è¯´æ˜ |
|------|------|-----------|-----------|------|
| ä¼ ç»Ÿ ASR | Whisper large-v3 | ~5% | ~45% | æ— é¢†åŸŸé€‚åº” |
| ä¼ ç»Ÿ ASR | Deepgram Nova-3 | ~4% | ~40% | å•†ç”¨æœåŠ¡ |
| **MLLM** | **GPT-4o** | ~3% | ~42% | ä½¿ç”¨ verbatim prompt |
| MLLM | Gemini 2.5 Pro | ~5% | ~48% | æ— æ˜æ˜¾æ”¹è¿› |

**å…³é”®å‘ç°**: 
- MLLM åœ¨é‡åº¦æ„éŸ³éšœç¢ä¸Šä»ç„¶è¡¨ç°ä¸ä½³
- GPT-4o çš„ verbatim prompt æŠ€å·§å€¼å¾—å°è¯•
- è¯­ä¹‰å¯æ¢å¤æ€§æ¯”å­—é¢å‡†ç¡®æ€§æ›´é‡è¦

---

## 3. å¤§è¯­è¨€æ¨¡å‹åœ¨æ„éŸ³éšœç¢ASRçš„åº”ç”¨

### 3.1 MLLM åå¤„ç†æ–¹æ¡ˆ

```python
# æ¦‚å¿µç¤ºæ„ - ä½¿ç”¨ LLM çº é”™
import openai

def asr_with_llm_correction(audio_path, asr_model, llm_client):
    # 1. ASR è¯†åˆ«
    raw_text = asr_model.transcribe(audio_path)
    
    # 2. LLM çº é”™
    prompt = f"""
    ä»¥ä¸‹æ˜¯æ„éŸ³éšœç¢æ‚£è€…çš„è¯­éŸ³è¯†åˆ«ç»“æœï¼Œå¯èƒ½å­˜åœ¨é”™è¯¯ã€‚
    è¯·æ ¹æ®ä¸Šä¸‹æ–‡è¯­ä¹‰è¿›è¡Œçº æ­£ï¼Œä¿ç•™åŸæ„ï¼š
    
    è¯†åˆ«ç»“æœ: {raw_text}
    çº æ­£å:
    """
    
    corrected = llm_client.complete(prompt)
    return corrected
```

### 3.2 å¤šæ¨¡æ€èåˆæ–¹å‘

```
æœªæ¥è¶‹åŠ¿:
Audio â†’ [ASR Encoder] â†’ Text Embedding
                              â†“
                        [LLM Decoder] â†’ çº æ­£åæ–‡æœ¬
                              â†‘
Context â†’ [Context Encoder] â†’ Context Embedding
```

---

## 4. ä¸»æµè®­ç»ƒç­–ç•¥å¯¹æ¯”

### 4.1 ç­–ç•¥å¯¹æ¯”è¡¨

| ç­–ç•¥ | ä¼˜ç‚¹ | ç¼ºç‚¹ | æ•°æ®éœ€æ±‚ | æ¨èåº¦ |
|------|------|------|----------|--------|
| **ç›´æ¥å¾®è°ƒ** | ç®€å•é«˜æ•ˆ | æ˜“è¿‡æ‹Ÿåˆ | ä¸­ | â­â­â­â­ |
| è¯¾ç¨‹å­¦ä¹  | æ¸è¿›é€‚åº” | éœ€è¦å¤šé˜¶æ®µ | é«˜ | â­â­â­ |
| æ•°æ®å¢å¼º | æ‰©å……æ•°æ® | å¯èƒ½å¼•å…¥å™ªå£° | ä½ | â­â­â­â­ |
| åŸå‹å­¦ä¹  | æ³›åŒ–æ€§å¥½ | å®ç°å¤æ‚ | ä¸­ | â­â­â­â­ |
| Speaker Adaptation | æ•ˆæœæœ€ä½³ | éœ€è¦æ¯äººæ•°æ® | æé«˜ | â­â­â­ |
| **å¯¹æ¯”å­¦ä¹ ** | ç‰¹å¾æ›´å¥½ | éœ€è¦è´Ÿæ ·æœ¬ | ä¸­ | â­â­â­â­â­ |

### 4.2 å„ç­–ç•¥è¯¦è§£

#### ç­–ç•¥1: ç›´æ¥å¾®è°ƒ (å½“å‰æ–¹æ¡ˆ)
```bash
# ä¼˜ç‚¹: ç®€å•ç›´æ¥
# ç¼ºç‚¹: å°æ•°æ®é›†æ˜“è¿‡æ‹Ÿåˆ
torchrun funasr/bin/train_ds.py \
++model="paraformer-large" \
++train_data_set_list="train.jsonl"
```

#### ç­–ç•¥2: æ•°æ®å¢å¼º (æ¨èæ·»åŠ )
```python
# FunASR å†…ç½® SpecAugment
# å¯é¢å¤–æ·»åŠ :
- Speed Perturbation: [0.9, 1.0, 1.1]
- Volume Perturbation
- ä¸å»ºè®®: è¿‡åº¦å™ªå£° (æ„éŸ³éšœç¢æœ¬èº«å°±æ˜¯"å™ªå£°")
```

#### ç­–ç•¥3: å¯¹æ¯”å­¦ä¹  (SOTAæ–¹æ³•)
```python
# åŸºäº HuBERT çš„ç›‘ç£å¯¹æ¯”å­¦ä¹ 
# å‚è€ƒ: arXiv:2407.18461
loss = CE_loss + Î» * Contrastive_loss
```

---

## 5. é’ˆå¯¹CDSD+Paraformerçš„ä¼˜åŒ–å»ºè®®

### 5.1 æ¨èç­–ç•¥ (ä¸æ”¹å˜å­¦ä¹ ç‡å’Œè½®æ¬¡)

åŸºäºæ–‡çŒ®è°ƒç ”ï¼Œæˆ‘ä»¬æ¨èä»¥ä¸‹ä¸¤ä¸ªä¼˜åŒ–ç­–ç•¥ï¼š

---

### â­ ç­–ç•¥A: æ•°æ®å¢å¼º (Speed Perturbation)

**åŸç†**: æ„éŸ³éšœç¢è¯­é€Ÿå¼‚å¸¸ï¼Œé€šè¿‡ speed perturbation å¢åŠ æ•°æ®å¤šæ ·æ€§

**å®ç°æ–¹å¼**: ä¿®æ”¹è®­ç»ƒé…ç½®

```yaml
# åœ¨ dataset_conf ä¸­æ·»åŠ 
dataset_conf:
  preprocessor_speech: SpeechPreprocessSpeedPerturb
  preprocessor_speech_conf:
    speed_perturb: [0.9, 1.0, 1.1]  # 0.9x, 1.0x, 1.1x ä¸‰ç§é€Ÿåº¦
```

**ä¿®æ”¹è®­ç»ƒå‘½ä»¤**:
```bash
torchrun --nproc_per_node=1 \
/root/CLEAR-VOX-MODEL/funasr/bin/train_ds.py \
++model="iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch" \
++train_data_set_list="/root/CLEAR-VOX-MODEL/data/1h_dataset/train.jsonl" \
++valid_data_set_list="/root/CLEAR-VOX-MODEL/data/1h_dataset/val.jsonl" \
++dataset="AudioDataset" \
++dataset_conf.batch_size=6000 \
++dataset_conf.batch_type="token" \
++dataset_conf.num_workers=4 \
++dataset_conf.preprocessor_speech="SpeechPreprocessSpeedPerturb" \
++dataset_conf.preprocessor_speech_conf.speed_perturb="[0.9, 1.0, 1.1]" \
++train_conf.max_epoch=50 \
++train_conf.log_interval=50 \
++train_conf.validate_interval=2000 \
++train_conf.keep_nbest_models=10 \
++train_conf.avg_nbest_model=10 \
++optim_conf.lr=0.0002 \
++output_dir="/root/CLEAR-VOX-MODEL/exp/paraformer_finetune_1h_sp"
```

**é¢„æœŸæ•ˆæœ**: 
- æ•°æ®é‡æ‰©å…… 3 å€
- CER é¢„æœŸä¸‹é™ 3-8%

---

### â­ ç­–ç•¥B: å¢å¼º SpecAugment (æ¨è)

**åŸç†**: æ›´å¼ºçš„é¢‘è°±é®è”½å¯ä»¥å¢å¼ºæ¨¡å‹é²æ£’æ€§

**Paraformer é»˜è®¤ SpecAug é…ç½®**:
```yaml
specaug_conf:
  freq_mask_width_range: [0, 30]
  num_freq_mask: 1
  time_mask_width_range: [0, 12]
  num_time_mask: 1
```

**å¢å¼ºé…ç½®** (å‚è€ƒå®˜æ–¹ç¤ºä¾‹):
```yaml
specaug_conf:
  freq_mask_width_range: [0, 30]
  num_freq_mask: 2        # å¢åŠ åˆ° 2
  time_mask_width_range: [0, 40]  # å¢åŠ 
  num_time_mask: 2        # å¢åŠ åˆ° 2
```

**ä¿®æ”¹è®­ç»ƒå‘½ä»¤**:
```bash
torchrun --nproc_per_node=1 \
/root/CLEAR-VOX-MODEL/funasr/bin/train_ds.py \
++model="iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch" \
++train_data_set_list="/root/CLEAR-VOX-MODEL/data/1h_dataset/train.jsonl" \
++valid_data_set_list="/root/CLEAR-VOX-MODEL/data/1h_dataset/val.jsonl" \
++dataset="AudioDataset" \
++dataset_conf.batch_size=6000 \
++dataset_conf.batch_type="token" \
++dataset_conf.num_workers=4 \
++specaug_conf.num_freq_mask=2 \
++specaug_conf.num_time_mask=2 \
++specaug_conf.time_mask_width_range="[0, 40]" \
++train_conf.max_epoch=50 \
++train_conf.log_interval=50 \
++train_conf.validate_interval=2000 \
++train_conf.keep_nbest_models=10 \
++train_conf.avg_nbest_model=10 \
++optim_conf.lr=0.0002 \
++output_dir="/root/CLEAR-VOX-MODEL/exp/paraformer_finetune_1h_aug"
```

**é¢„æœŸæ•ˆæœ**:
- å¢å¼ºæ¨¡å‹å¯¹é¢‘è°±å˜åŒ–çš„é²æ£’æ€§
- å¯¹æ„éŸ³éšœç¢çš„ä¸æ¸…æ™°å‘éŸ³æœ‰æ›´å¥½çš„æ³›åŒ–

---

### 5.2 æ¨èçš„å®Œæ•´è®­ç»ƒå‘½ä»¤ (ç»“åˆä¸¤ç§ç­–ç•¥)

```bash
#!/bin/bash
# ä¼˜åŒ–åçš„å¾®è°ƒè„šæœ¬ v2.1
# ç»“åˆ Speed Perturbation + å¢å¼º SpecAugment

export CUDA_VISIBLE_DEVICES="0"

workspace=/root/CLEAR-VOX-MODEL
model="iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch"

train_data="${workspace}/data/1h_dataset/train.jsonl"
val_data="${workspace}/data/1h_dataset/val.jsonl"
output_dir="${workspace}/exp/paraformer_finetune_1h_optimized"

mkdir -p ${output_dir}

echo "=============================================="
echo "FunASR Paraformer æ„éŸ³éšœç¢å¾®è°ƒ v2.1 (ä¼˜åŒ–ç‰ˆ)"
echo "=============================================="
echo "ç­–ç•¥: Speed Perturbation + å¢å¼º SpecAugment"
echo "=============================================="

torchrun --nproc_per_node=1 \
${workspace}/funasr/bin/train_ds.py \
++model="${model}" \
++train_data_set_list="${train_data}" \
++valid_data_set_list="${val_data}" \
++dataset="AudioDataset" \
++dataset_conf.index_ds="IndexDSJsonl" \
++dataset_conf.batch_sampler="BatchSampler" \
++dataset_conf.batch_size=6000 \
++dataset_conf.batch_type="token" \
++dataset_conf.num_workers=4 \
++dataset_conf.preprocessor_speech="SpeechPreprocessSpeedPerturb" \
++dataset_conf.preprocessor_speech_conf.speed_perturb="[0.9, 1.0, 1.1]" \
++specaug_conf.num_freq_mask=2 \
++specaug_conf.num_time_mask=2 \
++specaug_conf.time_mask_width_range="[0, 40]" \
++train_conf.max_epoch=50 \
++train_conf.log_interval=50 \
++train_conf.resume=true \
++train_conf.validate_interval=2000 \
++train_conf.save_checkpoint_interval=2000 \
++train_conf.keep_nbest_models=10 \
++train_conf.avg_nbest_model=10 \
++train_conf.use_deepspeed=false \
++optim_conf.lr=0.0002 \
++output_dir="${output_dir}" \
2>&1 | tee ${output_dir}/train.log

echo ""
echo "è®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜åœ¨: ${output_dir}"
```

---

## 6. å‚è€ƒæ–‡çŒ®

### æ ¸å¿ƒè®ºæ–‡

1. **Paraformer** (2022)
   - Gao et al. "Paraformer: Fast and Accurate Parallel Transformer for Non-autoregressive End-to-End Speech Recognition"
   - INTERSPEECH 2022
   - arXiv:2206.08317

2. **CDSD** (2024)
   - "CDSD: Chinese Dysarthria Speech Database"
   - INTERSPEECH 2024
   - arXiv:2310.15930

3. **Zero-Shot DSR with MLLM** (2025)
   - Alsayegh & Masood. "Zero-Shot Recognition of Dysarthric Speech Using Commercial ASR and Multimodal Large Language Models"
   - arXiv:2512.17474

4. **Prototype-Based Adaptation** (2024)
   - Wang et al. "Enhancing Dysarthric Speech Recognition for Unseen Speakers via Prototype-Based Adaptation"
   - INTERSPEECH 2024
   - arXiv:2407.18461

5. **DSR Fine-Tuning Strategies** (2024)
   - Leivaditi et al. "Fine-Tuning Strategies for Dutch Dysarthric Speech Recognition"
   - INTERSPEECH 2024

6. **DSR Overview** (2025)
   - Bhat & Strik. "Speech technology for automatic recognition and assessment of dysarthric speech: An overview"
   - Journal of Speech, Language, and Hearing Research

### ç›¸å…³æŒ‘æˆ˜èµ›

- **SLT 2024 LRDWWS Challenge**: Low-Resource Dysarthric Wake Word Spotting
- IEEE SLT 2024

---

## é™„å½•: CDSD æ•°æ®é›†ä¿¡æ¯

| å±æ€§ | å€¼ |
|------|-----|
| æ€»æ—¶é•¿ | 133 å°æ—¶ |
| è¯´è¯äººæ•° | 44 äºº |
| æœ€ä½³ CER | 16.4% (Hybrid CTC/Attention) |
| äººç±»åŸºçº¿ | 20.45% CER |
| ä¼šè®® | INTERSPEECH 2024 |

---

**æŠ¥å‘Šç»“æŸ**

*æœ¬æŠ¥å‘ŠåŸºäº 2025-12-23 çš„å…¬å¼€èµ„æ–™æ•´ç†*
